some text  Hi! I check the 5 steps of deriving EM algorithm in Assignment 3. I notice that step 5 asks us to write down $$p(Z | X, \Theta^{(t)})$$. However, this parameter has been used in step 2 already, since we take expectation w.r.t. $$Z \sim p(Z | X, \Theta^{(t)})$$.   My Question: Is it correct that for results in step 2-4, we should have result with unknown $$p(Z | X, \Theta^{(t)})$$ (or $$p(z_i | w_i, \Theta^{(t)})$$ for a single writer/word pair), and don't need to write down its explicit form? And we only write its explicit form in step 5?  Not a direct answer, but take a look at "EM Note 1" pdf from the schedule page. That should help. Thanks, I think it helps
Our experimental lab is going over due to inclusion of figures like confusion matrix and tables. Is this ok or should these things be omitted? I think they add a lot to the report personally. Thanks! No worries if you go over the length requirement  Thanks!
some text  After editing sampleseq3 (inserting a P) to  sampleseq4 do we have to retrain the model with sampleseq4?  On testing the insertion of P, Do we simply run: ./hmm -d -m sampleunsupmod3 -s sampleseq4  Or do we have to retrain HMM with sampleseq4? ./hmm -t -n 2 -m sampleunsupmod4 -s sampleseq4 ./hmm -d -m sampleunsupmod4 -s sampleseq4 yes! you need to retrain it
For the labelled documents, we enforce all words in the same document to be sampled from the same given topic.   However, for unlabelled documents, do we also assume that all words in the same document are from the same unknown topic? Or similar to standard PLSA we can assign different topics to different words?  yes, assuming that all words in the same document are from the same unknown topic means you chose case 1) from "tips on how to start". It is a valid assumption and will actually simplify some of the calculations. Wait, option 1 only enforces words in the same labeled documents to have same topic,  but says nothing about unlabeled documents.   Does the correct answer penalize assuming words from each unlabeled document follows the mixture distribution? nope no penalty. the rubric for this question is purposefully lenient because there are multiple ways to interpret it. but if you do assume that even the unlabeled documents' words come from only 1 topic, calculations will be simpler.
I've tried many different numbers, and all of them either tag all symbols to 0 or generate the same result as sampletag2 ;( Try changing output probability for state 0

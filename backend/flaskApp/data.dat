5 Steps to derive E-step and M-step TIPS ON HOW TO START: In general, we can derive an EM algorithm by following these 5 steps:  Step 1: Write down the complete likelihood function p(X, Z|@), which is usually simpler in the form than the incomplete likelihood p(X|@). Our original likelihood function is the incomplete likelihood, which is complex because of the sum over all the hidden variable values as Z is unknown/uncertain.  Step 2: Write down the Q-function, which is the expectation of the complete likelihood w.r.t. the distribution of hidden variables given (a) the observed data_X and (b) the current generation of parameters, denoted by @’. This distribution is in the form of WZ|X, 0’) and should be interpreted as a posterior distribution of Z after observing the data X (intuitively it represents our guess of the values of the hidden variables given the observed data, i.e., given an observed word, to what extent we believe that the word was written by H or T).  Step 3: Once you can write down the Q-function, take the partial derivative of the Q-function w.r.t. each parameter that you want to estimate (in our case, it's just the A ). Set the derivative to zero to form an equation.  Step 4: You should now have as many equations as the number of unknown parameters. Sometimes, our parameters may have constraints, e.g., the sum of the probabilities of all the words in a topic should be 1. In such a case, you'll have additional equations corresponding to those constraints. Solve the equations to obtain an updating formula for each unknown parameter. In our case, the equations can be solved analytically to obtain a closed formula for updating the parameter. These updating formulas are generally called M-step formulas.  Step 5: Write down the formula(s) for computing p(Z|X, ©’) using Bayes Rule. These formulas are generally called E-step formulas.  Hi! I check the 5 steps of deriving EM algorithm in Assignment 3. I notice that step 5 asks us to write down $$p(Z | X, \Theta^{(t)})$$. However, this parameter has been used in step 2 already, since we take expectation w.r.t. $$Z \sim p(Z | X, \Theta^{(t)})$$.   My Question: Is it correct that for results in step 2-4, we should have result with unknown $$p(Z | X, \Theta^{(t)})$$ (or $$p(z_i | w_i, \Theta^{(t)})$$ for a single writer/word pair), and don't need to write down its explicit form? And we only write its explicit form in step 5?  Not a direct answer, but take a look at "EM Note 1" pdf from the schedule page. That should help. Thanks, I think it helps
Experimental Lab Length Cutoff Our experimental lab is going over due to inclusion of figures like confusion matrix and tables. Is this ok or should these things be omitted? I think they add a lot to the report personally. Thanks! No worries if you go over the length requirement  Thanks!
HMM question 3.2 Can the HMM identify the boundaries between the DNA symbols and amino acid symbols? Now, change sampLeseq3 by inserting one "P" after the second "A" (counting from the beginning of the sequence), and try the procedure above again. Is the inserted "P" tagged as an amino acid or a nucleotide? Why? What if you insert six "P"s after the second "A"? How is the result different from when you insert just one "P"? Why?  After editing sampleseq3 (inserting a P) to  sampleseq4 do we have to retrain the model with sampleseq4?  On testing the insertion of P, Do we simply run: ./hmm -d -m sampleunsupmod3 -s sampleseq4  Or do we have to retrain HMM with sampleseq4? ./hmm -t -n 2 -m sampleunsupmod4 -s sampleseq4 ./hmm -d -m sampleunsupmod4 -s sampleseq4 yes! you need to retrain it
Assignment 3 Question 3 For the labelled documents, we enforce all words in the same document to be sampled from the same given topic.   However, for unlabelled documents, do we also assume that all words in the same document are from the same unknown topic? Or similar to standard PLSA we can assign different topics to different words?  yes, assuming that all words in the same document are from the same unknown topic means you chose case 1) from "tips on how to start". It is a valid assumption and will actually simplify some of the calculations. Wait, option 1 only enforces words in the same labeled documents to have same topic,  but says nothing about unlabeled documents.   Does the correct answer penalize assuming words from each unlabeled document follows the mixture distribution? nope no penalty. the rubric for this question is purposefully lenient because there are multiple ways to interpret it. but if you do assume that even the unlabeled documents' words come from only 1 topic, calculations will be simpler.
Any idea of HMM (Viterbi 1.3)? I've tried many different numbers, and all of them either tag all symbols to 0 or generate the same result as sampletag2 ;( Try changing output probability for state 0
